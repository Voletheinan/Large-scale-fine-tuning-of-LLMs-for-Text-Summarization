{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "2_FineTuning_Results.ipynb\n",
        "\n",
        "This notebook visualizes the training and validation loss curves for each fine-tuning method.\n",
        "It aims to show the learning progress and stability of different approaches during training.\n",
        "\"\"\"\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "\n",
        "from src.config import MODELS_DIR, FIGURES_DIR, REPORT_TITLE\n",
        "\n",
        "# Define paths to training logs\n",
        "LOG_DIRS = {\n",
        "    \"Full Fine-tuning\": os.path.join(MODELS_DIR, \"finetuned_full\", \"runs\"), # Adjust if logs are in a different structure\n",
        "    \"LoRA\": os.path.join(MODELS_DIR, \"finetuned_lora\", \"runs\"),\n",
        "    \"QLoRA\": os.path.join(MODELS_DIR, \"finetuned_qlora\", \"runs\"),\n",
        "    \"Adapter (IA3)\": os.path.join(MODELS_DIR, \"finetuned_adapter\", \"runs\"),\n",
        "    \"Prompt-tuning\": os.path.join(MODELS_DIR, \"finetuned_prompt_tuning\", \"runs\"),\n",
        "}\n",
        "\n",
        "# Function to extract logs (simplified, might need refinement based on actual log format)\n",
        "def extract_logs(log_dir: str) -> pd.DataFrame:\n",
        "    # Assuming logs are in `trainer_state.json` or similar format in the latest run directory\n",
        "    # This is a simplified approach and might need adjustment.\n",
        "    # A more robust solution would involve parsing event files or a custom logger.\n",
        "    log_files = glob.glob(os.path.join(log_dir, \"**/trainer_state.json\"), recursive=True)\n",
        "    if not log_files:\n",
        "        print(f\"No trainer_state.json found in {log_dir}. Trying other log formats...\")\n",
        "        # Fallback to a simpler log parsing if trainer_state.json is not available\n",
        "        # For example, if logs are printed to console and redirected to a file\n",
        "        # This part is highly dependent on how training logs are actually saved.\n",
        "        return pd.DataFrame() # Return empty if no known log format found\n",
        "    \n",
        "    # Get the latest log file if multiple exist\n",
        "    latest_log_file = max(log_files, key=os.path.getctime)\n",
        "    \n",
        "    try:\n",
        "        with open(latest_log_file, 'r', encoding='utf-8') as f:\n",
        "            trainer_state = json.load(f)\n",
        "        \n",
        "        # Extract relevant metrics (loss, eval_loss, epoch, step)\n",
        "        log_history = trainer_state.get('log_history', [])\n",
        "        df = pd.DataFrame(log_history)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading log file {latest_log_file}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "all_training_logs = {}\n",
        "for model_type, path in LOG_DIRS.items():\n",
        "    if os.path.exists(path):\n",
        "        logs = extract_logs(path)\n",
        "        if not logs.empty:\n",
        "            all_training_logs[model_type] = logs\n",
        "            print(f\"Extracted logs for {model_type}: {len(logs)} entries.\")\n",
        "        else:\n",
        "            print(f\"No valid logs extracted for {model_type}.\")\n",
        "    else:\n",
        "        print(f\"Log directory not found for {model_type}: {path}\")\n",
        "\n",
        "\n",
        "# --- Visualization: Training & Validation Loss ---\n",
        "if all_training_logs:\n",
        "    print(\"\\nGenerating Training and Validation Loss Plots...\")\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    \n",
        "    for model_type, logs_df in all_training_logs.items():\n",
        "        if 'loss' in logs_df.columns and 'eval_loss' in logs_df.columns and 'epoch' in logs_df.columns:\n",
        "            # Plot training loss\n",
        "            plt.plot(logs_df['epoch'], logs_df['loss'], label=f'{model_type} Training Loss', alpha=0.7)\n",
        "            # Plot validation loss (if available for all epochs)\n",
        "            # Note: eval_loss might not be logged for every step, only at evaluation_strategy epochs\n",
        "            eval_logs = logs_df.dropna(subset=['eval_loss'])\n",
        "            if not eval_logs.empty:\n",
        "                plt.plot(eval_logs['epoch'], eval_logs['eval_loss'], label=f'{model_type} Validation Loss', linestyle='--')\n",
        "            \n",
        "    plt.title(f'Training and Validation Loss Across Fine-tuning Methods ({REPORT_TITLE})', fontsize=16)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Loss', fontsize=12)\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.0)\n",
        "    plt.tight_layout()\n",
        "    output_path = os.path.join(FIGURES_DIR, \"training_validation_loss_curves.png\")\n",
        "    plt.savefig(output_path)\n",
        "    print(f\"Training and Validation Loss Plots saved to {output_path}\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No training logs found to generate plots.\")\n",
        "\n",
        "print(\"Fine-tuning results visualization setup complete. Run this notebook after training scripts.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
