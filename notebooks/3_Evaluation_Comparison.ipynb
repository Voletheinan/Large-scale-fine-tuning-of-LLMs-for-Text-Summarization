{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "3_Evaluation_Comparison.ipynb\n",
        "\n",
        "This notebook provides a comparative analysis of the evaluation results across different fine-tuning methods.\n",
        "It loads the ROUGE scores, training metrics, and other relevant data, and uses `src/visualize.py` to generate plots.\n",
        "\"\"\"\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Add parent directory to path\n",
        "sys.path.append('..')\n",
        "\n",
        "from src.config import TABLES_DIR\n",
        "from src.visualize import (\n",
        "    plot_rouge_scores, \n",
        "    plot_bleu_scores,\n",
        "    plot_trainable_parameters, \n",
        "    plot_training_and_inference_time,\n",
        "    plot_vram_usage,\n",
        "    plot_radar_chart,\n",
        "    plot_comprehensive_comparison\n",
        ")\n",
        "\n",
        "# --- Load Evaluation Results ---\n",
        "print(\"Loading evaluation results...\")\n",
        "results_path = os.path.join(TABLES_DIR, \"evaluation_results.csv\")\n",
        "\n",
        "if os.path.exists(results_path):\n",
        "    results_df = pd.read_csv(results_path)\n",
        "    print(\"Evaluation results loaded successfully.\")\n",
        "    print(f\"\\nEvaluation Results DataFrame ({len(results_df)} methods):\")\n",
        "    print(results_df.to_string(index=False))\n",
        "    \n",
        "    # Display summary statistics\n",
        "    print(\"\\n=== Summary Statistics ===\")\n",
        "    if 'ROUGE-L' in results_df.columns:\n",
        "        print(f\"\\nBest ROUGE-L: {results_df['ROUGE-L'].max():.4f} ({results_df.loc[results_df['ROUGE-L'].idxmax(), 'Method']})\")\n",
        "    if 'BLEU' in results_df.columns:\n",
        "        print(f\"Best BLEU: {results_df['BLEU'].max():.4f} ({results_df.loc[results_df['BLEU'].idxmax(), 'Method']})\")\n",
        "    if 'Training Time (min)' in results_df.columns:\n",
        "        print(f\"Fastest Training: {results_df['Training Time (min)'].min():.2f} min ({results_df.loc[results_df['Training Time (min)'].idxmin(), 'Method']})\")\n",
        "else:\n",
        "    print(f\"Error: Evaluation results file not found at {results_path}. Please run `src/evaluate.py` first.\")\n",
        "    results_df = pd.DataFrame()  # Create empty DataFrame to prevent errors\n",
        "\n",
        "# --- Generate Visualizations ---\n",
        "if not results_df.empty:\n",
        "    print(\"\\nGenerating comparative visualizations...\")\n",
        "    plot_rouge_scores(results_df)\n",
        "    plot_trainable_parameters(results_df)\n",
        "    plot_training_and_inference_time(results_df)\n",
        "    print(\"Visualizations generated and saved to report/figures.\")\n",
        "else:\n",
        "    print(\"Skipping visualization as evaluation results are empty.\")\n",
        "\n",
        "\n",
        "# --- Analysis and Discussion Points ---\n",
        "print(\"\\n--- Analysis and Discussion Points ---\")\n",
        "print(\"Based on the generated plots and the `results_df` above, consider the following for your report:\")\n",
        "print(\"1. **ROUGE Scores Comparison**: Which method achieved the highest ROUGE-1, ROUGE-2, and ROUGE-L scores? Discuss potential reasons. Are there trade-offs?\")\n",
        "print(\"2. **Trainable Parameters**: Compare the number of trainable parameters for each method. How does this relate to their performance and resource usage?\")\n",
        "print(\"3. **Training Time & Inference Time**: Analyze the training and inference times. Which methods are most efficient? Are there any unexpected results?\")\n",
        "print(\"4. **Resource Usage (GPU Memory)**: Although not explicitly plotted here (requires manual logging), discuss the GPU memory footprint of each method based on your observations during training.\")\n",
        "print(\"5. **Trade-offs**: Summarize the trade-offs between accuracy, training time, memory usage, and model complexity for each fine-tuning approach.\")\n",
        "print(\"6. **Conclusion for Report**: Which method would you recommend for this task under what constraints (e.g., limited GPU vs. high accuracy requirement)?\")\n",
        "\n",
        "print(\"Evaluation comparison complete. Review the generated figures in `report/figures/` for your report.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
