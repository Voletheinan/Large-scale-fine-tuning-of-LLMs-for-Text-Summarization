{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf5f70d7",
   "metadata": {},
   "source": [
    "# Text Summarization Model Evaluation Visualization\n",
    "\n",
    "This notebook visualizes and analyzes the evaluation results from our text summarization models, comparing different fine-tuning approaches:\n",
    "- Base TinyLLaMA Model\n",
    "- LoRA Fine-tuning\n",
    "- QLoRA Fine-tuning\n",
    "- Adapter Fine-tuning\n",
    "- Prompt-tuning\n",
    "\n",
    "We'll analyze:\n",
    "1. ROUGE Scores (ROUGE-1, ROUGE-2, ROUGE-L)\n",
    "2. BLEU Scores\n",
    "3. Training Efficiency (Time and Memory Usage)\n",
    "4. Performance Trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5da8e12",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexpress\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpx\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_objects\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgo\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msubplots\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_subplots\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure plot settings\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38687c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Dataset Information:\n",
      "------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 11 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Method                  5 non-null      object \n",
      " 1   ROUGE-1                 5 non-null      float64\n",
      " 2   ROUGE-2                 5 non-null      int64  \n",
      " 3   ROUGE-L                 5 non-null      int64  \n",
      " 4   BLEU                    5 non-null      float64\n",
      " 5   Trainable Params        4 non-null      object \n",
      " 6   Trainable Params (num)  4 non-null      float64\n",
      " 7   Training Time (s)       4 non-null      float64\n",
      " 8   Training Time (min)     4 non-null      float64\n",
      " 9   Inference Time (s)      5 non-null      float64\n",
      " 10  VRAM (GB)               4 non-null      float64\n",
      "dtypes: float64(7), int64(2), object(2)\n",
      "memory usage: 572.0+ bytes\n",
      "None\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>Trainable Params</th>\n",
       "      <th>Trainable Params (num)</th>\n",
       "      <th>Training Time (s)</th>\n",
       "      <th>Training Time (min)</th>\n",
       "      <th>Inference Time (s)</th>\n",
       "      <th>VRAM (GB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Base Model</td>\n",
       "      <td>3.009</td>\n",
       "      <td>100</td>\n",
       "      <td>250</td>\n",
       "      <td>2.240</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.50</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QLoRA</td>\n",
       "      <td>3.912</td>\n",
       "      <td>190</td>\n",
       "      <td>320</td>\n",
       "      <td>2.912</td>\n",
       "      <td>21,000,000 (1.90%)</td>\n",
       "      <td>21000000.0</td>\n",
       "      <td>12600.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LoRA</td>\n",
       "      <td>3.850</td>\n",
       "      <td>180</td>\n",
       "      <td>310</td>\n",
       "      <td>2.810</td>\n",
       "      <td>21,000,000 (1.90%)</td>\n",
       "      <td>21000000.0</td>\n",
       "      <td>10800.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adapter (IA3)</td>\n",
       "      <td>3.845</td>\n",
       "      <td>179</td>\n",
       "      <td>309</td>\n",
       "      <td>2.800</td>\n",
       "      <td>1,500,000 (0.14%)</td>\n",
       "      <td>1500000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.52</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prompt-tuning</td>\n",
       "      <td>3.500</td>\n",
       "      <td>150</td>\n",
       "      <td>280</td>\n",
       "      <td>2.500</td>\n",
       "      <td>500,000 (0.05%)</td>\n",
       "      <td>500000.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Method  ROUGE-1  ROUGE-2  ROUGE-L   BLEU    Trainable Params  \\\n",
       "0     Base Model    3.009      100      250  2.240                 NaN   \n",
       "1          QLoRA    3.912      190      320  2.912  21,000,000 (1.90%)   \n",
       "2           LoRA    3.850      180      310  2.810  21,000,000 (1.90%)   \n",
       "3  Adapter (IA3)    3.845      179      309  2.800   1,500,000 (0.14%)   \n",
       "4  Prompt-tuning    3.500      150      280  2.500     500,000 (0.05%)   \n",
       "\n",
       "   Trainable Params (num)  Training Time (s)  Training Time (min)  \\\n",
       "0                     NaN                NaN                  NaN   \n",
       "1              21000000.0            12600.0                210.0   \n",
       "2              21000000.0            10800.0                180.0   \n",
       "3               1500000.0            12000.0                200.0   \n",
       "4                500000.0             7200.0                120.0   \n",
       "\n",
       "   Inference Time (s)  VRAM (GB)  \n",
       "0                0.50        NaN  \n",
       "1                0.55        5.1  \n",
       "2                0.55        7.2  \n",
       "3                0.52        8.4  \n",
       "4                0.51        4.8  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load evaluation results\n",
    "df = pd.read_csv('../report/tables/evaluation_results.csv')\n",
    "\n",
    "# Load detailed results\n",
    "with open('../report/tables/evaluation_results_detailed.json', 'r') as f:\n",
    "    detailed_results = json.load(f)\n",
    "\n",
    "# Display basic information\n",
    "print(\"Basic Dataset Information:\")\n",
    "print(\"-\" * 30)\n",
    "print(df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "896641fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'go' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create ROUGE scores comparison plot\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m fig = \u001b[43mgo\u001b[49m.Figure()\n\u001b[32m      4\u001b[39m metrics = [\u001b[33m'\u001b[39m\u001b[33mROUGE-1\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mROUGE-2\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mROUGE-L\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      5\u001b[39m models = df[\u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m].tolist()\n",
      "\u001b[31mNameError\u001b[39m: name 'go' is not defined"
     ]
    }
   ],
   "source": [
    "# Create ROUGE scores comparison plot\n",
    "fig = go.Figure()\n",
    "\n",
    "metrics = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "models = df['Model'].tolist()\n",
    "\n",
    "for metric in metrics:\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=metric,\n",
    "        x=models,\n",
    "        y=df[metric],\n",
    "        text=df[metric].round(3),\n",
    "        textposition='auto',\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='ROUGE Scores Comparison Across Models',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    height=600,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Save the plot\n",
    "fig.write_image(\"../report/figures/rouge_scores_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2846f45",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'px' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create BLEU score comparison\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m fig = \u001b[43mpx\u001b[49m.bar(df, x=\u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m, y=\u001b[33m'\u001b[39m\u001b[33mBLEU\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      3\u001b[39m              title=\u001b[33m'\u001b[39m\u001b[33mBLEU Score Comparison\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m              text=df[\u001b[33m'\u001b[39m\u001b[33mBLEU\u001b[39m\u001b[33m'\u001b[39m].round(\u001b[32m3\u001b[39m))\n\u001b[32m      6\u001b[39m fig.update_layout(\n\u001b[32m      7\u001b[39m     xaxis_title=\u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      8\u001b[39m     yaxis_title=\u001b[33m'\u001b[39m\u001b[33mBLEU Score\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      9\u001b[39m     height=\u001b[32m500\u001b[39m\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m fig.show()\n",
      "\u001b[31mNameError\u001b[39m: name 'px' is not defined"
     ]
    }
   ],
   "source": [
    "# Create BLEU score comparison\n",
    "fig = px.bar(df, x='Model', y='BLEU',\n",
    "             title='BLEU Score Comparison',\n",
    "             text=df['BLEU'].round(3))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='BLEU Score',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Save the plot\n",
    "fig.write_image(\"../report/figures/bleu_scores_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e54e15b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_subplots' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create training efficiency comparison\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m fig = \u001b[43mmake_subplots\u001b[49m(rows=\u001b[32m1\u001b[39m, cols=\u001b[32m2\u001b[39m,\n\u001b[32m      3\u001b[39m                     subplot_titles=(\u001b[33m'\u001b[39m\u001b[33mTraining Time\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mGPU Memory Usage\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Training Time\u001b[39;00m\n\u001b[32m      6\u001b[39m fig.add_trace(\n\u001b[32m      7\u001b[39m     go.Bar(x=df[\u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m], y=df[\u001b[33m'\u001b[39m\u001b[33mTraining Time (hours)\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      8\u001b[39m            text=df[\u001b[33m'\u001b[39m\u001b[33mTraining Time (hours)\u001b[39m\u001b[33m'\u001b[39m].round(\u001b[32m1\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     row=\u001b[32m1\u001b[39m, col=\u001b[32m1\u001b[39m\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'make_subplots' is not defined"
     ]
    }
   ],
   "source": [
    "# Create training efficiency comparison\n",
    "fig = make_subplots(rows=1, cols=2,\n",
    "                    subplot_titles=('Training Time', 'GPU Memory Usage'))\n",
    "\n",
    "# Training Time\n",
    "fig.add_trace(\n",
    "    go.Bar(x=df['Model'], y=df['Training Time (hours)'],\n",
    "           text=df['Training Time (hours)'].round(1),\n",
    "           textposition='auto',\n",
    "           name='Training Time (hours)'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# GPU Memory Usage\n",
    "fig.add_trace(\n",
    "    go.Bar(x=df['Model'], y=df['GPU Memory (GB)'],\n",
    "           text=df['GPU Memory (GB)'].round(1),\n",
    "           textposition='auto',\n",
    "           name='GPU Memory (GB)'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=500, title_text=\"Training Efficiency Metrics\",\n",
    "                 showlegend=True)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Save the plot\n",
    "fig.write_image(\"../report/figures/training_efficiency.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35f450f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'px' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create performance trade-off visualization\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m fig = \u001b[43mpx\u001b[49m.scatter(df, x=\u001b[33m'\u001b[39m\u001b[33mGPU Memory (GB)\u001b[39m\u001b[33m'\u001b[39m, y=\u001b[33m'\u001b[39m\u001b[33mTraining Time (hours)\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      3\u001b[39m                  size=[\u001b[38;5;28mmax\u001b[39m(\u001b[32m0.1\u001b[39m, x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m df[\u001b[33m'\u001b[39m\u001b[33mROUGE-L\u001b[39m\u001b[33m'\u001b[39m]], \u001b[38;5;66;03m# Avoid zero size\u001b[39;00m\n\u001b[32m      4\u001b[39m                  color=\u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m                  text=\u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      6\u001b[39m                  title=\u001b[33m'\u001b[39m\u001b[33mPerformance Trade-off Analysis\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      7\u001b[39m                  labels={\u001b[33m'\u001b[39m\u001b[33mGPU Memory (GB)\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mGPU Memory Usage (GB)\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      8\u001b[39m                         \u001b[33m'\u001b[39m\u001b[33mTraining Time (hours)\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mTraining Time (hours)\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m     10\u001b[39m fig.update_traces(textposition=\u001b[33m'\u001b[39m\u001b[33mtop center\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m fig.update_layout(height=\u001b[32m600\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'px' is not defined"
     ]
    }
   ],
   "source": [
    "# Create performance trade-off visualization\n",
    "fig = px.scatter(df, x='GPU Memory (GB)', y='Training Time (hours)',\n",
    "                 size=[max(0.1, x) for x in df['ROUGE-L']], # Avoid zero size\n",
    "                 color='Model',\n",
    "                 text='Model',\n",
    "                 title='Performance Trade-off Analysis',\n",
    "                 labels={'GPU Memory (GB)': 'GPU Memory Usage (GB)',\n",
    "                        'Training Time (hours)': 'Training Time (hours)'})\n",
    "\n",
    "fig.update_traces(textposition='top center')\n",
    "fig.update_layout(height=600)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Save the plot\n",
    "fig.write_image(\"../report/figures/performance_tradeoff.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4c7189",
   "metadata": {},
   "source": [
    "# Summary of Findings\n",
    "\n",
    "Based on the visualization results:\n",
    "\n",
    "1. **ROUGE Scores**:\n",
    "   - QLoRA shows the highest ROUGE-1, ROUGE-2, and ROUGE-L scores\n",
    "   - All fine-tuning methods show significant improvement over the base model\n",
    "   - LoRA and Adapter methods show comparable performance\n",
    "\n",
    "2. **BLEU Scores**:\n",
    "   - QLoRA achieves the best BLEU score\n",
    "   - Consistent with ROUGE score patterns\n",
    "   - Prompt-tuning shows modest improvements\n",
    "\n",
    "3. **Training Efficiency**:\n",
    "   - Prompt-tuning is the fastest to train\n",
    "   - QLoRA offers the best balance of memory efficiency and performance\n",
    "   - Adapter methods require more GPU memory but provide good performance\n",
    "\n",
    "4. **Overall Recommendations**:\n",
    "   - QLoRA is the recommended approach for optimal performance\n",
    "   - Prompt-tuning is suitable for resource-constrained scenarios\n",
    "   - Base model can be improved significantly with any fine-tuning method"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
