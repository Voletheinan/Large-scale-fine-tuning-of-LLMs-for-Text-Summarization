{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "4_Sample_Predictions.ipynb\n",
        "\n",
        "This notebook demonstrates sample text summarization predictions from each fine-tuned TinyLLaMA model.\n",
        "It aims to provide a qualitative comparison of the summary quality across different fine-tuning methods.\n",
        "\"\"\"\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from src.config import RAW_DATA_DIR\n",
        "from src.prepare_data import load_raw_data\n",
        "from src.predict import load_model_for_prediction, generate_summary\n",
        "\n",
        "# --- Load Test Data ---\n",
        "print(\"Loading raw test dataset...\")\n",
        "test_df = load_raw_data(\"test\")\n",
        "print(\"Raw test dataset loaded successfully.\")\n",
        "\n",
        "# --- Select Sample Articles ---\n",
        "# You can choose specific indices or a random set\n",
        "sample_indices = [0, 1, 2] # Example indices\n",
        "# If you want random samples:\n",
        "# import random\n",
        "# random.seed(42)\n",
        "# sample_indices = random.sample(range(len(test_df)), 3)\n",
        "\n",
        "print(f\"\\nSelected sample indices for prediction: {sample_indices}\")\n",
        "\n",
        "# --- Define Models to Evaluate ---\n",
        "# Ensure these model types match the options in `src/predict.py`\n",
        "MODEL_TYPES = [\"full\", \"lora\", \"qlora\", \"adapter\", \"prompt_tuning\"]\n",
        "\n",
        "# Store loaded models to avoid reloading in the loop\n",
        "loaded_models = {}\n",
        "loaded_tokenizers = {}\n",
        "\n",
        "print(\"\\nLoading all fine-tuned models for prediction...\")\n",
        "for model_type in MODEL_TYPES:\n",
        "    try:\n",
        "        model, tokenizer = load_model_for_prediction(model_type)\n",
        "        loaded_models[model_type] = model\n",
        "        loaded_tokenizers[model_type] = tokenizer # Tokenizer should be the same, but store for consistency\n",
        "        print(f\"Loaded {model_type} model successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {model_type} model: {e}\")\n",
        "        loaded_models[model_type] = None # Mark as failed\n",
        "\n",
        "\n",
        "# --- Generate and Display Predictions ---\n",
        "print(\"\\n--- Generating and Displaying Sample Predictions ---\")\n",
        "for idx in sample_indices:\n",
        "    article = test_df.loc[idx, 'article']\n",
        "    reference_summary = test_df.loc[idx, 'summary']\n",
        "\n",
        "    print(f\"\\n===== Sample {idx} =====\")\n",
        "    print(f\"Original Article (truncated): {article[:500]}...\") # Truncate for display\n",
        "    print(f\"Reference Summary: {reference_summary}\")\n",
        "\n",
        "    for model_type in MODEL_TYPES:\n",
        "        if loaded_models[model_type] is not None:\n",
        "            print(f\"\\n--- {model_type.upper()} Generated Summary ---\")\n",
        "            # Ensure the correct tokenizer is used with the model\n",
        "            generated_summary = generate_summary(loaded_models[model_type], loaded_tokenizers[model_type], article)\n",
        "            print(f\"Generated Summary: {generated_summary}\")\n",
        "        else:\n",
        "            print(f\"\\n--- {model_type.upper()} Model (not loaded, skipping prediction) ---\")\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "\n",
        "print(\"Sample predictions generation complete. Review the outputs to compare summary quality.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
