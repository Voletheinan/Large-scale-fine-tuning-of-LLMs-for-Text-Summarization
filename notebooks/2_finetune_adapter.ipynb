{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "2_finetune_adapter.ipynb\n",
        "\n",
        "This notebook demonstrates how to fine-tune the LLaMA 3 model using the Adapter method (specifically, IA3) for text summarization.\n",
        "It reuses the 4-bit quantized base model and applies IA3 adapters on top.\n",
        "\"\"\"\n",
        "\n",
        "# Import necessary libraries\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
        "from peft import IA3Config, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define model name and paths\n",
        "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "BASE_MODEL_DIR = \"models/llama3_base/\"\n",
        "ADAPTER_MODEL_DIR = \"models/llama3_finetuned_adapter/\"\n",
        "DATA_DIR = \"data/\"\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(ADAPTER_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Load tokenizer (saved from previous step)\n",
        "print(f\"Loading tokenizer from {BASE_MODEL_DIR}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_DIR)\n",
        "print(\"Tokenizer loaded successfully.\")\n",
        "\n",
        "# Configure BitsAndBytes for 4-bit quantization (already done for base model, but redefine for clarity)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load base model with 4-bit quantization (this will load from Hugging Face if not cached)\n",
        "print(f\"Loading base model {MODEL_NAME} with 4-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "print(\"Base model loaded and quantized successfully.\")\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# IA3 (Adapter) configuration\n",
        "ia3_config = IA3Config(\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    feedforward_modules=[\"gate_proj\", \"up_proj\", \"down_proj\"], # Modules to apply IA3 to in feedforward\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Apply IA3 to the model\n",
        "model = get_peft_model(model, ia3_config)\n",
        "print(\"IA3 (Adapter) model prepared successfully.\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Load dataset\n",
        "print(\"Loading datasets...\")\n",
        "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
        "val_df = pd.read_csv(os.path.join(DATA_DIR, \"validation.csv\"))\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "print(\"Datasets loaded successfully.\")\n",
        "\n",
        "# Preprocess data\n",
        "MAX_LENGTH = 512\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = []\n",
        "    for article, summary in zip(examples['article'], examples['summary']):\n",
        "        instruction = f\"Summarize the following article: {article}\"\n",
        "        chat = [\n",
        "            {\"role\": \"user\", \"content\": instruction},\n",
        "            {\"role\": \"assistant\", \"content\": summary},\n",
        "        ]\n",
        "        inputs.append(tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False))\n",
        "\n",
        "    model_inputs = tokenizer(inputs, max_length=MAX_LENGTH, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
        "    return model_inputs\n",
        "\n",
        "print(\"Tokenizing and preprocessing datasets...\")\n",
        "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
        "print(\"Datasets tokenized and preprocessed successfully.\")\n",
        "\n",
        "tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"article\", \"summary\"])\n",
        "tokenized_val_dataset = tokenized_val_dataset.remove_columns([\"article\", \"summary\"])\n",
        "\n",
        "# Set up training arguments and trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=ADAPTER_MODEL_DIR, # Output directory for Adapter model\n",
        "    num_train_epochs=1, # Number of training epochs (for demo)\n",
        "    per_device_train_batch_size=2, # Batch size per GPU for training\n",
        "    per_device_eval_batch_size=2, # Batch size per GPU for evaluation\n",
        "    gradient_accumulation_steps=4, # Number of updates steps to accumulate before performing a backward/update pass\n",
        "    evaluation_strategy=\"epoch\", # Evaluation strategy to adopt during training\n",
        "    save_strategy=\"epoch\", # Save strategy to adopt during training\n",
        "    logging_dir=\"./logs_adapter\", # Directory for storing logs\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10, # Log every N updates steps\n",
        "    learning_rate=2e-4, # Learning rate\n",
        "    fp16=True, # Use half-precision (float16) for faster training\n",
        "    seed=42, # Random seed for reproducibility\n",
        "    report_to=\"none\", # Disable reporting to external services like Weights & Biases\n",
        "    remove_unused_columns=False, # Important for PEFT\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting Adapter fine-tuning (IA3)...\")\n",
        "trainer.train()\n",
        "print(\"Adapter fine-tuning (IA3) complete.\")\n",
        "\n",
        "# Save the fine-tuned IA3 adapter\n",
        "print(f\"Saving IA3 adapter to {ADAPTER_MODEL_DIR}...\")\n",
        "trainer.model.save_pretrained(ADAPTER_MODEL_DIR)\n",
        "print(\"IA3 adapter saved successfully.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
