{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "1_Data_Exploration.ipynb\n",
        "\n",
        "This notebook explores and provides statistics for the processed dataset used in the text summarization project.\n",
        "It loads the processed data, displays basic information, and analyzes text length distributions.\n",
        "\"\"\"\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Add parent directory to path\n",
        "sys.path.append('..')\n",
        "\n",
        "from src.prepare_data import load_processed_data\n",
        "from datasets import Dataset\n",
        "\n",
        "# --- Load Data ---\n",
        "print(\"Loading processed datasets...\")\n",
        "train_dataset = load_processed_data(\"train\")\n",
        "val_dataset = load_processed_data(\"validation\")\n",
        "test_dataset = load_processed_data(\"test\")\n",
        "\n",
        "# Convert to pandas for easier analysis\n",
        "train_df = train_dataset.to_pandas()\n",
        "val_df = val_dataset.to_pandas()\n",
        "test_df = test_dataset.to_pandas()\n",
        "\n",
        "print(\"Raw datasets loaded successfully.\")\n",
        "\n",
        "# Combine for overall statistics if needed, or analyze separately\n",
        "# For simplicity, we'll concatenate for length analysis here\n",
        "all_data_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
        "\n",
        "# --- Basic Statistics ---\n",
        "print(\"\\n--- Basic Statistics ---\")\n",
        "print(f\"Train dataset shape: {train_df.shape}\")\n",
        "print(f\"Validation dataset shape: {val_df.shape}\")\n",
        "print(f\"Test dataset shape: {test_df.shape}\")\n",
        "print(f\"Total data points: {all_data_df.shape[0]}\")\n",
        "\n",
        "print(\"\\n--- Sample Data (Train) ---\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"\\n--- Data Info (Train) ---\")\n",
        "train_df.info()\n",
        "\n",
        "# --- Text Length Analysis ---\n",
        "print(\"\\n--- Text Length Analysis ---\")\n",
        "all_data_df['article_len'] = all_data_df['article'].apply(lambda x: len(x.split()))\n",
        "all_data_df['summary_len'] = all_data_df['summary'].apply(lambda x: len(x.split()))\n",
        "\n",
        "print(\"\\nArticle Length Statistics (in words):\")\n",
        "print(all_data_df['article_len'].describe())\n",
        "\n",
        "print(\"\\nSummary Length Statistics (in words):\")\n",
        "print(all_data_df['summary_len'].describe())\n",
        "\n",
        "# --- Visualization: Length Distributions ---\n",
        "print(\"\\nGenerating length distribution plots...\")\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(all_data_df['article_len'], bins=50, color='skyblue')\n",
        "plt.title('Distribution of Article Lengths')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(all_data_df['summary_len'], bins=50, color='lightcoral')\n",
        "plt.title('Distribution of Summary Lengths')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save figures to report/figures\n",
        "figures_dir = os.path.join(\"report\", \"figures\")\n",
        "os.makedirs(figures_dir, exist_ok=True)\n",
        "plt.savefig(os.path.join(figures_dir, \"length_distributions.png\"))\n",
        "print(f\"Length distribution plots saved to {os.path.join(figures_dir, \"length_distributions.png\")}\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Data exploration complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
